{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ce58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MISTRAL_API_KEY'] = 'e7sPYrNGvXaeMFK8hgfjWb3lPSVOJrZP'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e6cd5e",
   "metadata": {},
   "source": [
    "## Prepare QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b29650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "import json\n",
    "def prepare_qa(file_path: str):\n",
    "    \"\"\"\n",
    "    Prepares QA pairs from a JSONL file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:  # Specify UTF-8 encoding\n",
    "        data = [json.loads(line) for line in file]\n",
    "\n",
    "    qa = []\n",
    "    for item in data:\n",
    "        qa.append({'question': item['question'], 'options': item['options'], 'answer': item['answer_idx']})\n",
    "        # qa.append({'question': item['question'], 'options': {\"A\": item['opa'],\"B\": item['opb'], \"C\": item['opc'], \"D\": item['opd']}, 'answer': chr(item['cop'] + 64) })\n",
    "\n",
    "    \n",
    "    return qa\n",
    "\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "516c8c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA = prepare_qa(r\"E:\\Git_clone\\RAG\\qa_dataset\\data_clean\\questions\\US\\4_options\\phrases_no_exclude_test.jsonl\")\n",
    "# QA = prepare_qa(r\"C:\\Users\\This PC\\Downloads\\medmcqa\\dev.json\")\n",
    "global count_ans\n",
    "count_ans = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088fb375",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02aafd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chatbot, importlib\n",
    "importlib.reload(chatbot)\n",
    "from chatbot import Chatbot\n",
    "chatbot = Chatbot(\"mistral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f987f",
   "metadata": {},
   "source": [
    "### Vector retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04b2bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vectordb\n",
    "importlib.reload(vectordb)\n",
    "from vectordb import create_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c3fc683",
   "metadata": {},
   "outputs": [],
   "source": [
    "vretriever = create_retriever(r\"E:\\workspaces\\YuE\\faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8439f1db",
   "metadata": {},
   "source": [
    "### Graph retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cf3c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphdb, importlib\n",
    "importlib.reload(graphdb)\n",
    "\n",
    "from graphdb import gretriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2f458db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, rag_type = None, k = 5):\n",
    "    if rag_type == None:\n",
    "        return \"\"\n",
    "    elif rag_type == \"rag\":\n",
    "        contexts = vretriever.get_relevant_documents(query, k=k)\n",
    "        return \"\\n\\n\".join([context.page_content for context in contexts])\n",
    "    elif rag_type == \"grag\":\n",
    "        vcontexts = vretriever.get_relevant_documents(query, k=k)\n",
    "        gcontexts = gretriever(\".\\n\".join([context.page_content for context in vcontexts]), extract_model=\"mistral\", k=999)\n",
    "        return \"\\n\".join([context.page_content for context in vcontexts]) + \"\\n\".join([context for context in gcontexts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50c47a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_qa(qa, rag_type = None, k = 5):\n",
    "    context = retrieve(qa['question'], rag_type, k=k)\n",
    "    prompt = f\"\"\"\n",
    "    You are a medical expert. Answer the question by using both the provided documents and your knowledge.\n",
    "    Question: {qa['question']}\n",
    "    Options:\n",
    "    A: {qa['options']['A']}\n",
    "    B: {qa['options']['B']}\n",
    "    C: {qa['options']['C']}\n",
    "    D: {qa['options']['D']}\n",
    "    Document: {context}\n",
    "    Answer: (\"[A]\", \"[B]\", \"[C]\", or \"[D]\"). DO NOT include the context, the explaination nor the question in your answer.\n",
    "    \"\"\"\n",
    "    response = chatbot.chat(prompt)\n",
    "    # print(\"Prompt: \", prompt, '\\n')\n",
    "    if len(response) == 1:\n",
    "        answer = response\n",
    "    if \"[A]\" in response:\n",
    "        answer = \"A\"\n",
    "    elif \"[B]\" in response:\n",
    "        answer = \"B\"\n",
    "    elif \"[C]\" in response:\n",
    "        answer = \"C\"\n",
    "    elif \"[D]\" in response:\n",
    "        answer = \"D\"\n",
    "    else:\n",
    "        answer = response[0]\n",
    "    \n",
    "    # print(response, \" \", answer, \"  \", qa['answer'])\n",
    "    output_file_path = \"output.txt\"  # Specify the output file path\n",
    "    with open(output_file_path, \"a\", encoding=\"utf-8\") as file:\n",
    "        file.write(\"1\" if answer == qa['answer'] else \"0\")\n",
    "\n",
    "    return answer == qa['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "980a82f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def process_qa_with_retry(qa, rag_type=None, retries=8, delay=1):\n",
    "    \"\"\"\n",
    "    Processes a single QA pair with retry logic for rate-limiting errors.\n",
    "\n",
    "    Parameters:\n",
    "        qa (dict): A single QA pair to process.\n",
    "        rag_type (str): Type of retriever to use (e.g., \"rag\", \"grag\").\n",
    "        retries (int): Number of retries for rate-limiting errors.\n",
    "        delay (int): Delay in seconds between retries.\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether the processed answer matches the expected answer.\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return process_qa(qa, rag_type)\n",
    "        except Exception as e:\n",
    "            time.sleep(delay)\n",
    "    return False  # Return False if all retries fail\n",
    "\n",
    "def process_qa_parallel(QA, n_workers=4, rag_type=None, k=5):\n",
    "    \"\"\"\n",
    "    Processes QA pairs in parallel and calculates accuracy.\n",
    "\n",
    "    Parameters:\n",
    "        QA (list): List of QA pairs to process.\n",
    "        n_workers (int): Number of worker threads to use.\n",
    "        rag_type (str): Type of retriever to use (e.g., \"rag\", \"grag\").\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the processed QA pairs.\n",
    "    \"\"\"\n",
    "    count_ans = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        # Submit tasks to the executor\n",
    "        futures = {executor.submit(process_qa_with_retry, qa, rag_type, k): qa for qa in QA}\n",
    "\n",
    "        # Use tqdm for progress tracking\n",
    "        for future in tqdm(as_completed(futures), total=len(QA), desc=\"Processing QA\"):\n",
    "            try:\n",
    "                if future.result():  # If the result is True, increment the counter\n",
    "                    count_ans += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing QA: {e}\")\n",
    "\n",
    "    accuracy = count_ans / len(QA)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b43a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  58%|█████▊    | 740/1273 [29:03<11:54,  1.34s/it]   "
     ]
    }
   ],
   "source": [
    "accuracy = process_qa_parallel(QA[:], n_workers=6, rag_type=\"rag\", k=8)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1f003c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7439120188531029"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e39eba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"1011110011100101010100101101111110010010000110111101110101010111111111101111001110111110101011101011\"\n",
    "cnt = 0x\n",
    "for i in s:\n",
    "    if i == \"1\":\n",
    "        cnt+=1\n",
    "cnt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
