{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ce58a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2e6cd5e",
   "metadata": {},
   "source": [
    "## Prepare QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b29650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "import json\n",
    "def prepare_qa(file_path: str):\n",
    "    \"\"\"\n",
    "    Prepares QA pairs from a JSONL file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:  # Specify UTF-8 encoding\n",
    "        data = [json.loads(line) for line in file]\n",
    "\n",
    "    qa = []\n",
    "    for item in data:\n",
    "        qa.append({'question': item['question'], 'options': item['options'], 'answer': item['answer_idx']})\n",
    "    \n",
    "    return qa\n",
    "\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "516c8c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA = prepare_qa(r\"E:\\Git_clone\\RAG\\qa_dataset\\data_clean\\questions\\US\\4_options\\phrases_no_exclude_test.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088fb375",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02aafd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chatbot, importlib\n",
    "importlib.reload(chatbot)\n",
    "from chatbot import Chatbot\n",
    "chatbot = Chatbot(\"mistral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f987f",
   "metadata": {},
   "source": [
    "### Vector retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04b2bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectordb import create_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c3fc683",
   "metadata": {},
   "outputs": [],
   "source": [
    "vretriever = create_retriever(r\"/workspaces/YuE/faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8439f1db",
   "metadata": {},
   "source": [
    "### Graph retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf3c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphdb import gretriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2f458db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, rag_type = None, k = 5):\n",
    "    if rag_type == None:\n",
    "        return \"\"\n",
    "    elif rag_type == \"rag\":\n",
    "        contexts = vretriever.get_relevant_documents(query, k=k)\n",
    "        return \"\\n\\n\".join([context.page_content for context in contexts])\n",
    "    elif rag_type == \"grag\":\n",
    "        contexts = vretriever.get_relevant_documents(query, k=k)\n",
    "        contexts = gretriever(\"\\n\".join([context.page_content for context in contexts]), extract_model=\"mistral\")\n",
    "        return \"\\n\".join([context for context in contexts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50c47a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_qa(qa, rag_type = None, k = 5):\n",
    "    context = retrieve(qa['question'], rag_type, k=k)\n",
    "    prompt = f\"\"\"\n",
    "    You are a medical expert. Answer the question by coorperate the provided context with your knowledgement.\n",
    "    Document: {context}\n",
    "    Question: {qa['question']}\n",
    "    Options:\n",
    "    A: {qa['options']['A']}\n",
    "    B: {qa['options']['B']}\n",
    "    C: {qa['options']['C']}\n",
    "    D: {qa['options']['D']}\n",
    "    Answer: (Only return A:, B:, C:, or D: without any explanation or other text. Do not include the context or the question in your answer.) \n",
    "    \"\"\"\n",
    "    response = chatbot.chat(prompt)\n",
    "    # print(\"Prompt: \", prompt, '\\n')\n",
    "    if \"A:\" in response:\n",
    "        answer = \"A\"\n",
    "    elif \"B:\" in response:\n",
    "        answer = \"B\"\n",
    "    elif \"C:\" in response:\n",
    "        answer = \"C\"\n",
    "    elif \"D:\" in response:\n",
    "        answer = \"D\"\n",
    "    else:\n",
    "        answer = None\n",
    "\n",
    "    # print(response,\" \", answer, \"  \", qa['answer'])\n",
    "    return answer == qa['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "980a82f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def process_qa_with_retry(qa, rag_type=None, retries=8, delay=1):\n",
    "    \"\"\"\n",
    "    Processes a single QA pair with retry logic for rate-limiting errors.\n",
    "\n",
    "    Parameters:\n",
    "        qa (dict): A single QA pair to process.\n",
    "        rag_type (str): Type of retriever to use (e.g., \"rag\", \"grag\").\n",
    "        retries (int): Number of retries for rate-limiting errors.\n",
    "        delay (int): Delay in seconds between retries.\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether the processed answer matches the expected answer.\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return process_qa(qa, rag_type)\n",
    "        except Exception as e:\n",
    "            if \"rate limit\" in str(e).lower():\n",
    "                # print(f\"Rate limit encountered. Retrying in {delay} seconds... (Attempt {attempt + 1}/{retries})\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                print(f\"Error processing QA: {e}\")\n",
    "                break\n",
    "    return False  # Return False if all retries fail\n",
    "\n",
    "def process_qa_parallel(QA, n_workers=4, rag_type=None, k=5):\n",
    "    \"\"\"\n",
    "    Processes QA pairs in parallel and calculates accuracy.\n",
    "\n",
    "    Parameters:\n",
    "        QA (list): List of QA pairs to process.\n",
    "        n_workers (int): Number of worker threads to use.\n",
    "        rag_type (str): Type of retriever to use (e.g., \"rag\", \"grag\").\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the processed QA pairs.\n",
    "    \"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        # Use tqdm for progress tracking\n",
    "        results = list(tqdm(\n",
    "            executor.map(lambda qa: process_qa_with_retry(qa, rag_type, k), QA),\n",
    "            total=len(QA),\n",
    "            desc=\"Processing QA\"\n",
    "        ))\n",
    "\n",
    "    # Calculate and return accuracy\n",
    "    total_correct = sum(results)\n",
    "    accuracy = total_correct / len(QA)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b43a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\This PC\\AppData\\Local\\Temp\\ipykernel_6284\\3995853207.py:8: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  contexts = vretriever.get_relevant_documents(query, k=k)\n",
      "Processing QA:   0%|          | 0/1273 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:   1%|          | 14/1273 [15:31<30:18:04, 86.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-29 14:54:44,393 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x000001F358D7E910>\n",
      "2025-04-29 14:54:44,396 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:   3%|▎         | 39/1273 [43:42<20:28:13, 59.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-29 15:22:56,390 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x000001F2BE17D7D0>\n",
      "2025-04-29 15:22:56,390 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:   4%|▍         | 50/1273 [54:24<19:59:20, 58.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-29 15:33:38,730 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x000001F357AD1DD0>\n",
      "2025-04-29 15:33:38,730 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:   4%|▍         | 55/1273 [1:01:01<26:35:27, 78.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-29 15:40:15,629 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x000001F2F0C91B50>\n",
      "2025-04-29 15:40:15,629 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:   4%|▍         | 57/1273 [1:03:43<25:53:30, 76.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-29 15:42:57,704 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x000001F358E50650>\n",
      "2025-04-29 15:42:57,704 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:   6%|▌         | 73/1273 [1:24:02<31:19:52, 93.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-29 16:03:14,917 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x000001F35F121DD0>\n",
      "2025-04-29 16:03:14,917 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:   6%|▌         | 79/1273 [1:30:40<20:01:51, 60.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-29 16:09:53,157 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x000001F35FC8C050>\n",
      "2025-04-29 16:09:53,158 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:   6%|▋         | 81/1273 [1:32:50<20:46:31, 62.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-29 16:12:04,073 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x000001F357AD1DD0>\n",
      "2025-04-29 16:12:04,074 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:   7%|▋         | 86/1273 [1:40:45<30:56:18, 93.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-29 16:19:58,034 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x000001F353BB7350>\n",
      "2025-04-29 16:19:58,034 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:   7%|▋         | 91/1273 [1:46:10<22:16:04, 67.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-29 16:25:23,669 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x000001F353BB7350>\n",
      "2025-04-29 16:25:23,669 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:   7%|▋         | 94/1273 [1:48:21<16:39:43, 50.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-29 16:27:33,871 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x000001F2B73E94D0>\n",
      "2025-04-29 16:27:33,872 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:   9%|▊         | 109/1273 [2:01:06<17:38:35, 54.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-29 16:40:19,440 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x000001F2E6283CD0>\n",
      "2025-04-29 16:40:19,440 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:   9%|▊         | 110/1273 [2:03:43<27:30:09, 85.13s/it]"
     ]
    }
   ],
   "source": [
    "accuracy = process_qa_parallel(QA, n_workers=1, rag_type=\"grag\", k=8)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f003c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7250589159465829"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
