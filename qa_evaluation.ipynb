{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f3ce58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "os.environ['MISTRAL_API_KEY'] = '8oOC7DNMAact4JocfZnzgZjlR4d8ogu9'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e6cd5e",
   "metadata": {},
   "source": [
    "## Prepare QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b29650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "import json\n",
    "def prepare_qa(file_path: str):\n",
    "    \"\"\"\n",
    "    Prepares QA pairs from a JSONL file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:  # Specify UTF-8 encoding\n",
    "        data = [json.loads(line) for line in file]\n",
    "\n",
    "    qa = []\n",
    "    for item in data:\n",
    "        qa.append({'question': item['question'], 'options': item['options'], 'answer': item['answer_idx']})\n",
    "    \n",
    "    return qa\n",
    "\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "516c8c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA = prepare_qa(r\"E:\\Git_clone\\RAG\\qa_dataset\\data_clean\\questions\\US\\4_options\\phrases_no_exclude_test.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088fb375",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02aafd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chatbot, importlib\n",
    "importlib.reload(chatbot)\n",
    "from chatbot import Chatbot\n",
    "chatbot = Chatbot(\"mistral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f987f",
   "metadata": {},
   "source": [
    "### Vector retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04b2bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectordb import create_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c3fc683",
   "metadata": {},
   "outputs": [],
   "source": [
    "vretriever = create_retriever(r\"/workspaces/YuE/faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8439f1db",
   "metadata": {},
   "source": [
    "### Graph retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf3c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphdb, importlib\n",
    "importlib.reload(graphdb)\n",
    "\n",
    "from graphdb import gretriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f458db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, rag_type = None, k = 5):\n",
    "    if rag_type == None:\n",
    "        return \"\"\n",
    "    elif rag_type == \"rag\":\n",
    "        contexts = vretriever.get_relevant_documents(query, k=k)\n",
    "        return \"\\n\\n\".join([context.page_content for context in contexts])\n",
    "    elif rag_type == \"grag\":\n",
    "        vcontexts = vretriever.get_relevant_documents(query, k=4)\n",
    "        gcontexts = gretriever(\".\\n\".join([context.page_content for context in vcontexts]), extract_model=\"mistral\", k=k)\n",
    "        return \"\\n\".join([context.page_content for context in vcontexts]) + \"\\n\".join([context for context in gcontexts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c47a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_qa(qa, rag_type = None, k = 5):\n",
    "    context = retrieve(qa['question'], rag_type, k=k)\n",
    "    prompt = f\"\"\"\n",
    "    You are a medical expert. Answer the question by coorperate the provided documents with your inner knowledgement.\n",
    "    Document: {context}\n",
    "    Question: {qa['question']}\n",
    "    Options:\n",
    "    A: {qa['options']['A']}\n",
    "    B: {qa['options']['B']}\n",
    "    C: {qa['options']['C']}\n",
    "    D: {qa['options']['D']}\n",
    "    Answer: (Only return A:, B:, C:, or D: .Do not include the context or the question in your answer.) \n",
    "    \"\"\"\n",
    "    response = chatbot.chat(prompt)\n",
    "    # print(\"Prompt: \", prompt, '\\n')\n",
    "    if \"A:\" in response:\n",
    "        answer = \"A\"\n",
    "    elif \"B:\" in response:\n",
    "        answer = \"B\"\n",
    "    elif \"C:\" in response:\n",
    "        answer = \"C\"\n",
    "    elif \"D:\" in response:\n",
    "        answer = \"D\"\n",
    "    else:\n",
    "        answer = None\n",
    "    \n",
    "    output_file_path = \"output.txt\"  # Specify the output file path\n",
    "    with open(output_file_path, \"a\", encoding=\"utf-8\") as file:\n",
    "        file.write(\"1\" if answer == qa['answer'] else \"0\")\n",
    "\n",
    "    # print(response,\" \", answer, \"  \", qa['answer'])\n",
    "    return answer == qa['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "980a82f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def process_qa_with_retry(qa, rag_type=None, retries=8, delay=1):\n",
    "    \"\"\"\n",
    "    Processes a single QA pair with retry logic for rate-limiting errors.\n",
    "\n",
    "    Parameters:\n",
    "        qa (dict): A single QA pair to process.\n",
    "        rag_type (str): Type of retriever to use (e.g., \"rag\", \"grag\").\n",
    "        retries (int): Number of retries for rate-limiting errors.\n",
    "        delay (int): Delay in seconds between retries.\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether the processed answer matches the expected answer.\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return process_qa(qa, rag_type)\n",
    "        except Exception as e:\n",
    "            if \"rate limit\" in str(e).lower():\n",
    "                # print(f\"Rate limit encountered. Retrying in {delay} seconds... (Attempt {attempt + 1}/{retries})\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                print(f\"Error processing QA: {e}\")\n",
    "                break\n",
    "    return False  # Return False if all retries fail\n",
    "\n",
    "def process_qa_parallel(QA, n_workers=4, rag_type=None, k=5):\n",
    "    \"\"\"\n",
    "    Processes QA pairs in parallel and calculates accuracy.\n",
    "\n",
    "    Parameters:\n",
    "        QA (list): List of QA pairs to process.\n",
    "        n_workers (int): Number of worker threads to use.\n",
    "        rag_type (str): Type of retriever to use (e.g., \"rag\", \"grag\").\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the processed QA pairs.\n",
    "    \"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        # Use tqdm for progress tracking\n",
    "        results = list(tqdm(\n",
    "            executor.map(lambda qa: process_qa_with_retry(qa, rag_type, k), QA),\n",
    "            total=len(QA),\n",
    "            desc=\"Processing QA\"\n",
    "        ))\n",
    "\n",
    "    # Calculate and return accuracy\n",
    "    total_correct = sum(results)\n",
    "    accuracy = total_correct / len(QA)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "003b43a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:   2%|▏         | 3/200 [01:39<1:44:29, 31.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 11:48:46,611 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013BD8914C90>\n",
      "2025-05-04 11:48:46,618 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n",
      "2025-05-04 11:49:24,749 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013C705CFB90>\n",
      "2025-05-04 11:49:24,750 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:   2%|▎         | 5/200 [03:53<2:41:59, 49.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 11:51:00,354 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013C70CE5FD0>\n",
      "2025-05-04 11:51:00,356 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:   8%|▊         | 17/200 [07:25<57:03, 18.71s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 11:54:32,491 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013BD884AC50>\n",
      "2025-05-04 11:54:32,491 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  16%|█▌        | 31/200 [14:17<1:24:57, 30.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 12:01:24,407 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013B7EBEC610>\n",
      "2025-05-04 12:01:24,408 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  21%|██        | 42/200 [18:31<46:03, 17.49s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 12:05:38,671 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013C6D525850>\n",
      "2025-05-04 12:05:38,672 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  23%|██▎       | 46/200 [20:23<55:56, 21.79s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 12:07:30,095 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013C703877D0>\n",
      "2025-05-04 12:07:30,096 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n",
      "2025-05-04 12:08:21,241 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013BE366EA90>\n",
      "2025-05-04 12:08:21,242 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n",
      "2025-05-04 12:08:22,614 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013BE799EE50>\n",
      "2025-05-04 12:08:22,616 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  25%|██▌       | 50/200 [22:23<1:06:27, 26.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 12:09:30,432 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013BD3A8FDD0>\n",
      "2025-05-04 12:09:30,433 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  30%|███       | 61/200 [28:09<1:10:38, 30.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 12:15:16,089 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013B3036A090>\n",
      "2025-05-04 12:15:16,090 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  37%|███▋      | 74/200 [36:23<1:33:04, 44.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 12:23:30,279 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013BE19E1450>\n",
      "2025-05-04 12:23:30,279 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  39%|███▉      | 78/200 [38:15<1:08:53, 33.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 12:25:23,007 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013C6DACDD10>\n",
      "2025-05-04 12:25:23,008 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  42%|████▎     | 85/200 [41:41<58:19, 30.43s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 12:28:47,664 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013C70CE1AD0>\n",
      "2025-05-04 12:28:47,665 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  43%|████▎     | 86/200 [42:10<57:18, 30.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 12:29:16,951 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013B000AE690>\n",
      "2025-05-04 12:29:16,952 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  44%|████▎     | 87/200 [42:39<56:20, 29.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 12:29:46,073 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013B76BF8110>\n",
      "2025-05-04 12:29:46,073 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  46%|████▋     | 93/200 [45:05<41:51, 23.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 12:32:12,428 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013B05EB86D0>\n",
      "2025-05-04 12:32:12,429 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  52%|█████▏    | 103/200 [48:26<32:02, 19.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 12:35:32,713 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013BD9F6A590>\n",
      "2025-05-04 12:35:32,714 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  54%|█████▍    | 108/200 [50:20<29:40, 19.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 12:37:27,016 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013C703EDC50>\n",
      "2025-05-04 12:37:27,016 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  56%|█████▌    | 112/200 [52:30<38:50, 26.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 12:40:12,536 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013B2E9B5390>\n",
      "2025-05-04 12:40:12,537 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  60%|██████    | 120/200 [55:47<28:18, 21.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 12:42:54,091 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013B4B59F1D0>\n",
      "2025-05-04 12:42:54,092 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  62%|██████▎   | 125/200 [58:17<36:36, 29.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 12:46:00,186 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013C6FB28750>\n",
      "2025-05-04 12:46:00,187 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  71%|███████   | 142/200 [1:07:24<35:54, 37.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 12:54:30,726 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013B26E590D0>\n",
      "2025-05-04 12:54:30,727 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  78%|███████▊  | 156/200 [1:13:14<22:37, 30.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 13:01:07,467 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013BE359A650>\n",
      "2025-05-04 13:01:07,469 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  78%|███████▊  | 157/200 [1:14:01<25:31, 35.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 13:01:09,061 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013B2E9B5390>\n",
      "2025-05-04 13:01:09,062 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  82%|████████▎ | 165/200 [1:17:53<16:34, 28.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 13:05:00,048 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013B76BF8110>\n",
      "2025-05-04 13:05:00,050 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  84%|████████▍ | 169/200 [1:20:14<17:47, 34.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 13:07:20,939 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013B000AE690>\n",
      "2025-05-04 13:07:20,939 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n",
      "2025-05-04 13:08:01,977 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013B30891D50>\n",
      "2025-05-04 13:08:01,978 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  86%|████████▌ | 171/200 [1:21:37<18:00, 37.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 13:08:44,108 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013BD9E9A450>\n",
      "2025-05-04 13:08:44,109 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  90%|████████▉ | 179/200 [1:24:09<07:51, 22.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 13:11:40,101 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013C00C3A350>\n",
      "2025-05-04 13:11:40,103 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 250, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 256, in handle_request\n",
      "    raise exc from None\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 236, in handle_request\n",
      "    response = connection.handle_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 103, in handle_request\n",
      "    return self._connection.handle_request(request)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 136, in handle_request\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 106, in handle_request\n",
      "    ) = self._receive_response_headers(**kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 177, in _receive_response_headers\n",
      "    event = self._receive_event(timeout=timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 231, in _receive_event\n",
      "    raise RemoteProtocolError(msg)\n",
      "httpcore.RemoteProtocolError: Server disconnected without sending a response.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 219, in complete\n",
      "    http_res = self.do_request(\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\basesdk.py\", line 277, in do_request\n",
      "    http_res = do()\n",
      "               ^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\basesdk.py\", line 245, in do\n",
      "    raise e\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\basesdk.py\", line 238, in do\n",
      "    http_res = client.send(req, stream=stream)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_client.py\", line 1014, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 249, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 155, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.RemoteProtocolError: Server disconnected without sending a response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  90%|█████████ | 180/200 [1:24:33<07:35, 22.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing QA: Unable to process messages: the only provided model did not run successfully. Error: Server disconnected without sending a response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  93%|█████████▎| 186/200 [1:27:26<06:36, 28.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 13:14:33,502 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013BDF0C8990>\n",
      "2025-05-04 13:14:33,503 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA:  98%|█████████▊| 195/200 [1:32:04<03:19, 39.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 13:19:10,971 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.mistral_model.MistralModel object at 0x0000013C6DBEFC10>\n",
      "2025-05-04 13:19:10,974 - camel.agents.chat_agent - ERROR - An error occurred while running model mistral-large-latest, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\agents\\chat_agent.py\", line 777, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\base_model.py\", line 191, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\camel\\models\\mistral_model.py\", line 252, in _run\n",
      "    response = self._client.chat.complete(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\This PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistralai\\chat.py\", line 243, in complete\n",
      "    raise models.SDKError(\n",
      "mistralai.models.sdkerror.SDKError: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA: 100%|██████████| 200/200 [1:33:44<00:00, 28.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = process_qa_parallel(QA[:200], n_workers=2, rag_type=\"grag\", k=999)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1f003c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc7b53f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000111100111001101111101011101111000010100010110100111001101000000101101101110110101011111010000111']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1274\u001b[39m, \u001b[38;5;241m1274\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1250\u001b[39m):\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlines\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      7\u001b[0m         cnt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(cnt \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1273\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "with open(\"output.txt\", 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "print(lines)\n",
    "cnt = 0\n",
    "for i in range(1274, 1274+1250):\n",
    "    if lines[0][i] == '1':\n",
    "        cnt += 1\n",
    "\n",
    "\n",
    "print(cnt / 1273)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
